---
title: "Cyclistic Data Analysis Report"
author: "Edgardo Rojas"
date: "1/9/2022"
output:
  cleanrmd::html_document_clean:
    theme: new.css
    toc: true
    toc_float: true

---

### How does a bike share navigate speedy success?

#### About the company...
Cyclistic is a bike sharing service operating in the city of Chicago. It has a bike
fleet of almost 6 thousand, and has over 690 stations across the city.

Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments.
One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes,
and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers
who purchase annual memberships are Cyclistic members.

Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the
pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will
be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a
very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic
program and have chosen Cyclistic for their mobility needs.

### The business task
As one of the main parts of the data analysis cycle represents having a good understanding of the business need, it's key to state the business task. In this case it is to gather insights into how the different user types use Cyclistic's bike service differently.

## How do annual members and casual riders use Cyclistic bikes   differently?

To start this analysis, I'm going to be using first party data, which is the data that Cyclistic has collected over the year, and that has been made public by **Motivate International Inc.** under [this](https://ride.divvybikes.com/data-license-agreement) license. It is public and free for anyone to use.

This data is stored on Cyclistic cloud servers, and is organized by month and by year quarter, in this case I will only use the last twelve month's data to do this analysis.

Since this is first party data, there are no issues from bias, security or credibility.

## Data analysis
To start off, I'm going to be using Rstudio and I'll load the required libraries. The most used ones are going to be the **tidyverse**, **lubridate**, **dplyr** and **ggplot2** for visualizations.

```{r Rstudio Setup, message=FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
library(reshape2)
library(cleanrmd)
```

### Loading the data
The next step is to load the data. The original data is stored in csv documents, which are coma separated values. We need to store all these values in a data frame, in order to be able to perform this analysis.
I'll load all 12 month's data into separate data frames first, which is important to do to actually make sure it's consistent. It's important to do this first, to then actually be able to join them for ease of analysis.

```{r Loading the data, message=FALSE}

January_2021 <- read.csv("202101-divvy-tripdata.csv", header = TRUE)
February_2021 <- read.csv("202102-divvy-tripdata.csv", header = TRUE)
March_2021 <- read.csv("202103-divvy-tripdata.csv", header = TRUE)
April_2021 <- read.csv("202104-divvy-tripdata.csv", header = TRUE)
May_2021 <- read.csv("202105-divvy-tripdata.csv", header = TRUE)
June_2021 <- read.csv("202106-divvy-tripdata.csv", header = TRUE)
July_2021 <- read.csv("202107-divvy-tripdata.csv", header = TRUE)
August_2021 <- read.csv("202108-divvy-tripdata.csv", header = TRUE)
September_2021 <- read.csv("202109-divvy-tripdata.csv", header = TRUE)
October_2021 <- read.csv("202110-divvy-tripdata.csv", header = TRUE)
November_2021 <- read.csv("202111-divvy-tripdata.csv", header = TRUE)
December_2021 <- read.csv("202112-divvy-tripdata.csv", header = TRUE)


```

### Examining the data
After loading the data I'll take a look at all the column names and column data types
to make sure it's consistent.

```{r Standardizing column names}
colnames(January_2021)
colnames(February_2021)
colnames(March_2021)
colnames(April_2021)
colnames(May_2021)
colnames(June_2021)
colnames(July_2021)
colnames(August_2021)
colnames(September_2021)
colnames(October_2021)
colnames(November_2021)
colnames(December_2021)
```

This looks good so I'll check the data types.

```{r Data types, message=FALSE}
str(January_2021)
str(February_2021)
str(March_2021)
str(April_2021)
str(May_2021)
str(June_2021)
str(July_2021)
str(August_2021)
str(September_2021)
str(October_2021)
str(November_2021)
str(December_2021)
```
The "str" function provides very useful metadata and is generally very helpful to get a glance of the data.
In this case I see that some columns were imported as character type, which will definitely cause issues
if this is not properly taken care of and transformed into something that we can make calculations with.

I'll first join all these different month's dataframes into one.

```{r Joining the data}

all_trips2021 <- bind_rows(January_2021, February_2021, March_2021, April_2021, May_2021, June_2021,       July_2021, August_2021, September_2021, October_2021, November_2021, December_2021)

```

I'll take a quick look at the merged dataset...

```{r Full dataset quick peak}
colnames(all_trips2021)
str(all_trips2021)
glimpse(all_trips2021)

```

The summary function is also very helpful, especially when looking at the factor columns, that is, columns that have integer data types.

```{r Summary}

summary(all_trips2021)

```

### Data Cleaning and Data Transformation
Since I'm not going to be doing analysis on our customers locations or distance traveled, It's better to drop the columns that are not useful to me now..

```{r Dropping columns}
all_trips2021 <- select(all_trips2021, -c(start_lat, start_lng, end_lat, end_lng))

```

Now that I have only the columns that I need, I will cast the started_at and ended_at columns into a datetime type.

```{r Transforming columns into datetime type}
all_trips2021 <- mutate(all_trips2021, started_at = ymd_hms(started_at), ended_at = ymd_hms(ended_at))

```

Confirming this data type transformation worked correctly.

```{r Confirming mutate}
str(all_trips2021)

```

Now that I've confirmed it did change the data types from characters to POSIXct format, which I can use, I want to look at the data as a whole again, now using the summary function. I did see earlier some emptpy values in the end_station_name column which I want to also investigate now...

```{r Data summary}
summary(all_trips2021)

all_trips2021 %>%
  summarise(count = sum(is.na(end_station_name)))
all_trips2021 %>%
  summarise(count = sum(end_station_name==""))

```

So interestingly the **is.na** function doesn't recognize missing values so I summed up the rows that contained empty spaces and a lot were missing.

After reviewing the data, it seems that the station data is not whole, and has very inconsistent values, but with having so much data, this might not be an issue. 

### Data Validation
I'll do some data validation on a couple columns now to make sure that the data I need is complete and clean.

```{r Data Validation}

sum(duplicated(all_trips2021$ride_id)) 
## This confirms there are no repeated rows since ride_id's are unique.
unique(all_trips2021$rideable_type)  
## This confirms there's only 3 rideable_types and no typos.
unique(all_trips2021$member_casual) 
## Confirming there are only 2 possible user type values.

```

### Transforming the data
So after making sure the data I have is clean, I'll proceed to create a couple more columns with extracted information from the started_at and ended_at columns. The reason for this, is to have more information about the usage of the bikes separating it by months, day of the week and even calculate hourly usage to see how it varies throughout the day. I'll also first calculate the duration from every ride by measuring the difference in time from the start to the end of the ride.

```{r Transforming the data}
# This calculates the time difference in seconds.
all_trips2021 <-  mutate(all_trips2021, ride_duration = as.numeric(difftime(ended_at, started_at)))

# I'll check the ride duration column to see if there are negative values or zero values. Which I'll then #delete.
table(sign(all_trips2021$ride_duration))

#So there is 147 negative ride_duration values and 506 zero values. I need to take care of both 
# of these. From here on now, these wrong values will be ignored.

filtered_trips <- subset(all_trips2021, between(ride_duration, 1, 15000)) 
str(filtered_trips)
```

This filtered data is based on distribution plotting to find the range where most of the values  lie. I found that the normal distribution was between 1 and 15000 seconds, the values above
this range I consider to be outliers, most likely software errors in time entries, bikes stolen or decommissioned, etc.

Now I have the proper ride duration data in seconds and have validated that the values are correct.
Then comes extracting the hour, day of week and month.

```{r Extracting datetime data}
filtered_trips <- mutate(filtered_trips, day_of_week = wday(started_at, week_start = getOption("lubridate.week.start",1)), ride_start_hour = hour(started_at))

head(filtered_trips)

```

## Analyzing and visualizing ride data
Before I start developing insights I want to get a proper understanding of the ride duration values. I want to see the distribution. This will hopefully reveal different characteristics between casual and member subscribers.

```{r Ride duration analyzing and visualization}

## I'll first look into the ride duration times with a histogram since doing a scatter plot with this much data is a bad idea (learned that the hard way).
## This first line of code is to stop R from displaying the Y axis in scientific notation
options(scipen=1000000)
ggplot(data=filtered_trips)+
  geom_histogram(mapping = aes(x=ride_duration, fill = member_casual), binwidth = 300)+
  facet_wrap(~member_casual)+
  labs(title="Ride duration distribution per user type",x="Ride duration in seconds", y="Number of rides")+
   scale_fill_discrete(name = "User Type")

```
Outliers make ploting this distribution plot difficult. If you font remove the extreme values it doesn't represent the distribution correctly, which is why I experimented with removing data past 15,000 seconds. 

This visualization alone offers so much insight into the characteristics of the user types.
We can clearly observe that there is a lot more members who take short rides than casual users,
we can also see a tendency of casual users to ride for longer duration, whereas members almost never take rides for longer than 5000 seconds (83 minutes).

It would be useful to keep track of a possible key metric, which would be average ride duration for both user types.

```{r Average ride duration per user type}

average_duration_df <- filtered_trips %>% 
  group_by(member_casual, day_of_week) %>% 
  summarise(average_ridetime=mean(ride_duration))

head(average_duration_df)


ggplot(average_duration_df, aes(x=day_of_week, y=average_ridetime, fill=member_casual)) +
  geom_col(stat='identity', position='dodge') +
  labs(title="Average ride time for both type of users per day", x="Day of the week", y="Average ride time in seconds.")


```

This graph shows that on average casual members ride twice the time and distance than members.
This information could be useful to make further assumptions on the differences between member and casual riders differences.

Another useful thing would be to know what bikes are the most used. For this I'll make a calculation and a quick bar plot.

```{r Rideable type percentages}
unique(filtered_trips$rideable_type)
#It is known that there are only 3 types of bikes. I'll now calculate what percentages of user types are which bikes.

rideable_type_df <- filtered_trips %>% 
  group_by(rideable_type, member_casual) %>% 
  summarise(number_of_rides=n_distinct(ride_id))

ggplot(rideable_type_df, aes(rideable_type, number_of_rides, fill=rideable_type)) +
  geom_col() +
  facet_wrap(~member_casual) +
  labs(title="Member type bike preferences in 2021", x="Type of bike", y="Number of bike rides")


```

I think it would be interesting to also see the usage of bikes as the week goes by.

```{r Week day anaylisis}
## Since I've already extracted the day of the week off of the started_at datetime column,
## all I have to do is to calculate and summarize these values.

subscriber_ride_count <- filtered_trips %>% 
  group_by(day_of_week) %>% 
  filter(member_casual=="member") %>% 
  summarise(member_rides=n_distinct(ride_id))

customer_ride_count <- filtered_trips %>% 
  group_by(day_of_week) %>% 
  filter(member_casual=="casual") %>% 
  summarise(casual_rides=n_distinct(ride_id))

usage_per_day_df = merge(customer_ride_count,subscriber_ride_count, by="day_of_week")
head(usage_per_day_df)




```

Now I have a small dataframe with the calculated values of rides per day. I will convert this dataframe first, in order to have the appropiate form, from which I can plot the data easier, using the reshape library.

```{r Visualizing the day distribution per user type}
options(scipen=1000000)
usage_per_day_df2 <- melt(usage_per_day_df, id.vars='day_of_week')
  head(usage_per_day_df2)
  
usage_per_day_df2 <- mutate(usage_per_day_df2, 
      day_of_week_string=recode(usage_per_day_df2$day_of_week, 
       "1"="Monday",
       "2"="Tuesday",
       "3"="Wednesday",
       "4"="Thursday",
       "5"="Friday",
       "6"="Saturday",
       "7"="Sunday"))


ggplot(usage_per_day_df2, aes(x=reorder(day_of_week_string, day_of_week), y=value, fill=variable)) +
    geom_bar(stat='identity', position='dodge') +
    labs(title = "Yearly usage of bikes per day in 2021 for Member and Casual users", x="Day of the week", y="Number of rides") + theme(axis.text.x = element_text(angle = 45, vjust = 1.0, hjust=1))
```

This bar plot reveals some usage differences between the casual and the member riders.

It shows that the casual riders tend to be more active during the weekend, where as the member riders average around the same rides per day for the whole week with a dip on Sunday.

This does not mean that the usage is the same during the week, we have to analyze how the usage varies throughout the day to get more insight in to how member and casual riders use the Cyclistic service.

We have already established that members generally take much shorter rides than casual riders and that casual riders are more active during the weekend.

I think it would be useful to do an analysis on usage troughout the day to figure out more information about the members, even though we're more interested in converting casual riders into members.

We could still benefit a lot by diving deeper into the usage characteristics of the different member types to achieve the business task.

I'll now plot the usage throughout the day. Since I already took care of extracting the hour of the day from the start_time column, all I have to do is plot the data.

```{r Usage troughout the day}

ggplot(data=filtered_trips) +
  geom_bar(mapping = aes(x=ride_start_hour, fill=member_casual), position="dodge") + labs(title = "Daily usage of bikes per day in 2021 for Member and Casual users", x="Hour of the day", y="Number of rides")


```

This plot shows that there is large spikes in member rides from 5AM to 8AM and from 4PM to 6PM, leading me to believe that those come from commuters, who use the bike sharing service to go to work.

These spikes are not surprising even though the City of Chicago is not particularly bike friendly, according to the City Ratings program developed by People for Bikes, a non profit. Chicago received a score of 16 out of 100, ranking it in the bottom 5 percent of large cities.  

I'll plot the same data now with lines, to display better the difference in ridership.

```{r Line graph}
subscriber_hourly_trips <- filtered_trips %>% 
  group_by(ride_start_hour) %>% 
  filter(member_casual=="member") %>% 
  summarise(hourly_member_rides=n_distinct(ride_id))

customer_hourly_trips <- filtered_trips %>% 
  group_by(ride_start_hour) %>% 
  filter(member_casual=="casual") %>% 
  summarise(hourly_casual_rides=n_distinct(ride_id))

hourly_trips_df = merge(subscriber_hourly_trips, customer_hourly_trips, by="ride_start_hour")
head(hourly_trips_df)

hourly_trips_df2 <- melt(hourly_trips_df, id.vars='ride_start_hour')
  head(hourly_trips_df2)
  
ggplot(hourly_trips_df2, aes(x=ride_start_hour, y = value, group = variable, colour = variable)) + geom_line(size=1.5) +
  geom_point(size=3.5) +
  labs(title="Hourly usage of bikes per member types", x="Hour of the day", y="Number of rides", fill="Member types")

```

This better demonstrates the peaks of usage. We can see that both the lines for members and casual riders
peak around 4PM and 6PM which is when people start going home from their place of work. From this I can gather that there is large unrealized potential in casual riders who use the Cyclistic service after leaving their place of work, most likely to go home. It is probable that some of these casual commuters haven't upgraded their subscription and could benefit from doing so.

Combining this knowledge with knowing that casual riders ride for far longer duration and subsequently, longer distances, I can start developing ideas to entice casual riders to upgrade to the member subscription. 

## Conclusions

A lot of information has come out of this analysis, but I will save the cleaned data to a .csv file aswell as the auxiliary data frames I created to make visualizations easier. I'll then use these csv files to visualize the data in tableau to make a dashboard for the stakeholders.

```{r Saving the data}
directory <- getwd()
directory

write.csv(filtered_trips, "C:/Users/Sergio Laptop/Desktop/Capstone Project/Cyclsitic Data Analysis/filtered_trips.csv")
write.csv(average_duration_df, "C:/Users/Sergio Laptop/Desktop/Capstone Project/Cyclsitic Data Analysis")
write.csv(usage_per_day_df2, "C:/Users/Sergio Laptop/Desktop/Capstone Project/Cyclsitic Data Analysis")
write.csv(hourly_trips_df2, "C:/Users/Sergio Laptop/Desktop/Capstone Project/Cyclsitic Data Analysis")

```





